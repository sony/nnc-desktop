/* Copyright 2024 Sony Group Corporation. */
/**
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
*/

const tooltip_en = {
    network: {
        component: {
            IO: 'Input/output data.',
            input: 'This is the neural network input layer.',
            output: 'Network Output.',
            Basic: 'Basic operations of artificial neurons.',
            affine: 'The affine layer is a fully-connected layer that has connections from all inputs to all output neurons specified with the OutShape property.',
            convolution: 'The convolution layer is used to convolve the input.',
            // locallyConnectedLayer: 'Convolution with Sparse coding only; often following MaxPooling',
            Pooling: 'Pooling layers.',
            maxPooling: 'MaxPooling outputs the maximum value of local inputs.',
            averagePooling: 'AveragePooling outputs the average of local inputs.',
            sumPooling: 'SumPooling outputs the sum of local inputs. ',
            unpooling: 'Unpooling copies a single input to multiple inputs in order to generate data larger in size than the input data.',
            upsampling: 'Upsampling.',
            Activation: 'Activation function with nonlinear transformation.',
            tanh: 'Tanh outputs the result of taking the hyperbolic tangent of the input.',
            sigmoid: 'Sigmoid outputs the result of taking the sigmoid of the input.',
            reLU: 'ReLU outputs the result of applying the rectified linear unit (ReLU) to the input.',
            cReLU: 'Concatenated ReLU (CReLU) applies Relu to each of the negated input signals',
            pReLU: 'Unlike ReLU, which always outputs 0 for inputs less than 0',
            eLU: 'ELU outputs the result of applying the exponential linear unit (ELU) to the input.',
            cELU: 'Concatenated ELU (CELU) applies ELU to each of the negated input signals, ',
            abs: 'Abs outputs the absolute values of inputs.',
            maxout: 'Piecewise linear function approximating any convex functions.',
            softmax: 'Softmax outputs the Softmax of inputs.',
            binary: 'Weights are binarized to reduce parameters and computation.',
            binaryConnectAffine: 'BinaryConnectAffine is an Affine layer that uses W, which has been converted into the binary values of -1 and +1.',
            binaryConnectConvolution: 'BinaryConnectConvolution is a convolution layer that uses W, which has been converted into the binary values of -1 and +1.',
            binaryWeightAffine: 'BinaryWeightAffine is an affine layer that uses W, which has been converted into binary values of -1 and +1, and then scaled in order to make the output closer to the normal Affine layer.',
            binaryWeightConvolution: 'BinaryWeightConvolution is an affine layer that uses W, which has been converted into binary values of -1 and +1, and then scaled to make the output closer to the normal Convolution layer.',
            xnorNetAffine: 'BinaryConnectConvolution with approximation factors.',
            xnorNetConvolution: 'BinaryWeightConvolution with input of 1/-1 and approximation factors.',
            binaryTanh: 'Binarytanh outputs -1 for inputs less than or equal to 0 and +1 for inputs great than 0.',
            binarySigmoid: 'BinarySigmoid outputs 0 for inputs less than or equal to 0 and +1 for inputs great than 0.',
            Math: 'Operations on tensors.',
            product: 'Take a product element-wisely.',
            sum: 'Sum determines the sum of the values of the specified dimension.',
            round: 'Round rounds the input value.',
            not: 'One minus the input.',
            "Arithmetic (Scalar)": 'Arithmetic operations with scalars.',
            addScalar: 'Add the internal value to the input.',
            mulScalar: 'Multiply the input by the internal value.',
            subSucalar: 'Subutract the internal value from the input.',
            rSubSucalar: 'Subtract the input from the internal value.',
            divScalar: 'Devide the input by the internal value.',
            rDivScalar: 'Devide the internal vlaue by the input.',
            powScalar: 'Power the input to the internal value.',
            rPowScalar: 'Power the internal value to the input.',
            maximumScalar: 'Maximum the input and the internal value.',
            minimumScalar: 'Minimum the input and the internal value.',
            "Arithmetic (2 Inputs)": 'Binary operations.',
            add2: 'Addition of two inputs.',
            sub2: 'Subtract the second input from the first input.',
            mul2: 'Multiply the two inputs.',
            div2: 'Devide the first input by the second input.',
            pow2: 'Power the first input to the second input.',
            maximum2: 'Maximum the two inputs.',
            minimum2: 'Minimum the two inputs.',
            Others: 'Other functions such as tensor processing.',
            batchNormalization: 'The input is normalized to 0 mean and 1 variance.',
            dropout: 'Dropout sets input elements to 0 with a given probability.',
            dropmap: 'Drop each sample in a batch, or batch-wise dropout.',
            embedding: 'Embed a discrete value like a character or word to vectorize it.',
            localResponseNormalization: 'Normalize each regions in zero-mean, unit-variance',
            reshape: 'Reshape transforms the shape of an array into the specified shape.',
            flatten: 'Flatten dimensions.',
            concate: 'Concatenate layers used for multimodal inputs or network once forked.',
            transpose: 'Transpose swaps data dimensions.',
            slice: 'Slice extracts part of the array.',
            reverse: 'Reverse the values on the given axis.',
            Parameter: 'Neural network parameters.',
            LoopControl: 'Describe the iteration process.',
            Quantize: 'Quantization and computation of artificial neurons with quantization.',
            Unit: 'Used for units that combine multiple layers.',
            Loss: 'Loss function used for optimization.',
            "Others(Pre Process)": 'Data preprocessing functions.',
            Setting: 'Configure network settings.',
            Logical: 'Logical operations.',
            Validation: 'Accuracy evaluation functions.',
            Trigonometric: 'Trigonometric functions.',
            Spectral: 'Conversion between time series data and frequency data.',
            parameter: 'This is a neural network parameter.',
            squaredError: 'This is the output layer of a neural network that minimizes the squared errors between the variables and dataset variables.',
            huberLoss: 'This is the output layer of a neural network that minimizes the Huber loss between the variables and dataset variables.',
            absoluteError: 'This is the output layer of a neural network that minimizes absolute error between the variables and dataset variables.',
            epsilonInsensitiveLoss: 'This is the output layer of a neural network that minimizes absolute error exceeding the range specified by Epsilon between the variables and dataset variables.',
            binaryCrossEntropy: 'This is the output layer of a neural network that minimizes the mutual information between the variable and dataset variables.',
            sigmoidCrossEntropy: 'This is the output layer of a neural network that minimizes the cross entropy between the variables and dataset variables.',
            categoricalCrossEntropy: 'This is the output layer of a neural network that minimizes the cross entropy between the variables and dataset variables provided by the category index.',
            softmaxCrossEntropy: 'This is the output layer of a neural network that minimizes the cross entropy between the variables and dataset variables provided by the category index. ',
            kLMultinomial: 'This is the output layer of a neural network that minimizes the Kullback Leibler distance between the probability distribution(p).',
            workingMemory: 'This is a buffer for temporarily storing computation results.',
            depthwiseConvolution: 'The convolution layer is used to convolve the input for each map.',
            deconvolution: 'The deconvolution layer is used to deconvolve the input.',
            depthwiseDeconvolution: 'The DepthwiseDeconvolution layer is used to deconvolve the input for each map.',
            embed: 'Inputs are assumed to be discrete symbols represented by integers ranging from 0 to N-1 (where N is the number of classes).',
            globalAveragePooling: 'GlobalAveragePooling outputs the average of the entire last two dimensions of an array.',
            leakyReLU: 'LeakyReLU multiplies inputs less than 0 with a constant value to output results.',
            sELU: 'SELU outputs the result of applying the scaled exponential linear unit (SELU) to the input.',
            swish: 'Swish outputs the result of taking the swish of the input.',
            repeatStart: 'RepeatStart is a layer that indicates the start position of a loop.',
            repeatEnd: 'RepeatEnd is a layer that indicates the end position of a loop.',
            recurrentInput: 'RecurrentInput is a layer that indicates the time loop start position of a recurrent neural network.',
            recurrentOutput: 'RecurrentOutput is a layer that indicates the time loop end position of a recurrent neural network.',
            delay: 'Delay is a layer that indicates the time delay signal in a recurrent neural network.',
            fixedPointQuantize: 'FixedPointQuantize performs linear quantization.',
            pow2Quantize: 'Pow2Quantize performs power-of-two quantization.',
            argument: 'Set the parameters of the unit network to allow editing as unit properties from the caller network.',
            fFT: 'Performs a Fourier transform of the complex input and complex output. The last dimension of the input array must be complex (real part and imaginary part).',
            iFFT: 'Performs an inverse Fourier transform of the complex input and complex output.',
            mean: 'Mean determines the average of the values of the specified dimension.',
            prod: 'Prod determines the product of the values of the specified dimension.',
            max: 'Max determines the maximum of the values of the specified dimension.',
            min: 'Min determines the minimum of the values of the specified dimension.',
            log: 'Log calculates the natural logarithm with base e.',
            exp: 'Exp calculates the exponential function with base e.',
            sign: 'Sign outputs -1 for negative inputs, +1 for positive inputs, and alpha for 0.',
            batchMatmul: 'BatchMatmul calculates the matrix multiplication of the matrix expressed by the last two dimensions of input (A) and connector R input (B).',
            ceil: 'Ceil rounds up the fraction of the input value.',
            floor: 'Floor truncates the fraction of the input value.',
            sin: 'o=sin(i)',
            cos: 'o=cos(i)',
            tan: 'o=tan(i)',
            sinh: 'o=sinh(i)',
            cosh: 'o=cosh(i)',
            aSin: 'o=arcsin(i)',
            aCos: 'o=arccos(i)',
            aTan: 'o=arctan(i)',
            aSinh: 'o=asinh(i)',
            aCosh: 'o=asinh(i)',
            aTanh: 'o=atanh(i)',
            // addScalar: 'o=i+value',
            // mulScalar: 'o=i*value',
            // rSubScalar: 'o=value-i',
            // rDivScalar:	'o=value/i',
            // powScalar: 'o=i value',
            // rPowScalar:	'o=value i',
            // maximumScalar: 'o=max (i,value)',
            // minimumScalar: 'o=min(i,value)',
            // add2: 'o=i1+i2',
            // sub2: 'o=i1-i2',
            // mul2: 'o=i1*i2',
            // div2: 'o=i1/i2 Connect the input for i2 in the right hand side to connector R.',
            // pow2: 'o=i1i2 Connect the input for i2 in the right hand side to connector R.',
            // maxmum2: 'o=max(i1,i2)',
            // minimum2: 'o=min(i1,i2)',
            logicalAnd:	'o= i1 and i2',
            logicalOr: 'o= i1 or i2',
            logicalXor:	'o= i1 xor i2',
            equal: 'o= i1 == i2',
            notEqual: 'o= i1 != i2',
            greaterEqual: 'o= i1 >= i2',
            greater: 'o= i1 > i2',
            lessEqual: 'o= i1 <= i2',
            less: 'o= i1 < i2',
            logicalAndScalar: 'o= i and value',
            logicalOrScalar: 'o= i or value',
            logicalXorScalar: 'o= i xor value',
            equalScalar: 'o= i == value',
            notEqualScalar:	'o= i != value',
            greaterEqualScalar:	'o= i >= value',
            greaterScalar: 'o= i > value',
            lessEqualScalar: 'o= i <= value',
            lessScalar:	'o= i < value',
            logicalNot:	'o= !i',
            binaryError: 'The input data and variable T in the dataset indicating correct values are converted into binary values (0 or 1) depending on whether the values are greater than or equal to 0.5.',
            topNError: 'Based on the input data indicating the probability or score of each category and variable T in the dataset indicating the category index.',
            concatenate: 'Concatenate joins two or more arrays on an existing axis.',
            broadcast: 'Broadcast transforms the dimension of an array whose number of elements is 1 to the specified size.',
            pad: 'Pad adds an element with the specified size to each dimension of a array.',
            flip: 'Flip reverses the order of elements of the specified dimension of an array.',
            shift: 'Shift shifts the array elements by the specified amount.',
            stack: 'Stack joins two or more arrays on a new axis.',
            matrixDiag: 'MatrixDiag performs a matrix diagonalization of the last one dimension of an array.',
            matrixDiagPart: 'MatrixDiagPart extracts the diagonal component of the last two dimensions of an array.',
            clipGradByValue: 'ClipGradByValue fits the gradient value within the specified range.',
            clipGradByNorm: 'ClipGradByValue fits the gradient value within the specified range.',
            topKData: 'TopKData retains K values in order from the largest data included in the input and sets the other values to zero.',
            topKGrad: 'TopKGrad retains K values in order from the largest gradient and sets the other values to zero.',
            sort: 'Data is sorted according the magnitude of the value.',
            prune: 'Sets values to zero in order from the smallest absolute value.',
            interpolate: 'Data is expanded or reduce through interpolation.',
            vATNoise: 'This layer is provided to achieve a method called Virtual Adversarial Training.',
            unlink: 'The input is output directly during forward calculation, and zero is output during backward calculation.',
            identity: 'Identity outputs the input as-is.',
            oneHot: 'OneHot creates one-hot array based on input indices.',
            meanSubtraction: 'MeanSubtraction normalizes input to mean 0.',
            randomFlip: 'RandomFlip reverses the order of elements of the specified dimension of an array at 50% probability.',
            randomShift: 'RandomShift randomly shifts the array elements within the specified range.',
            randomCrop: 'RandomCrop randomly extracts a portion of an array.',
            imageAugmentation: 'ImageAugmentation randomly alters the input image.',
            structureSearch: 'StructureSearch is used to configure the automatic structure search.',
            unit: 'Inserts a unit network into the current caller network.',
            comment: 'This is the layer for inserting comments into the network graph.',
            lSTM: 'LSTM unit is an experimentally implemented unit.',
            broadcastTo: 'This layer is provided for compatibility with Neural Network Libraries.',
            nmsDetection2d: 'This layer is provided for compatibility with Neural Network Libraries.'
        },
        layer: {
            inputDataset: 'Input a correspondence of the header of data in Neural Network Console CSV Format. Basically, x.',
            inputSize: 'Tensor dimension. E.g, 3,height,width for color image.',
            outputInput: 'Use the same value as the num. of classes. E.g., in 10 classes classification problem, use 10 for adjusting output value of the previous layers; You may use Affine to Softmax so that you can adjust output value of Affine layer.',
            outputLossFunction: 'Normally, select CategoricalCrossEntropy for classification and L2Squared for regression.',
            outputDataset: 'Input a correspondence of the header of labels in Neural Network Console CSV Format. Basically, y.'
        }
    },
    dataset: {
        trainingButton: 'Dataset for training.',
        validationButton: 'Dataset for validation.',
        uploadData: 'Data upload. Use "Download Utility Tools" to compress your data, then upload those. You can upload files up to 1GB.',
        downloadUploader: 'Uploader to s3 and data compressor utility.'
    },
    config: {
        data: {
            batchSize: 'Unit of samples fed into the network.'
        },
        environment: {
            local: 'Select when learning task executed in a host where Neural Network Console is hosted.',
            aws: 'Select when Neural Network Console is hosted in AWS and executing learning task in another instance.',
            processorType: 'Select GPU if you can use. When using AWS, select GPU.',
            cpu: 'Select when to train with CPU.',
            gpu: 'Select when to train with GPU.',
            multi_gpu: 'Select when to train with multiple GPUs',
            single: 'Select usually.'
        },
        optimizer: {
            baseLearningRate: 'Initial value for learning rate.',
            decayParameter: 'Decay parameter.',
            learningRateControlMethod: 'Algorithm to determine learning rate of SGD.',
            default: 'Stochastic gradient.',
            momentum: 'Current gradient is considered more preferable than decaying past gradients.',
            adagrad: 'Adaptive leaning rate.',
            adadelta: 'Adaptive leaning rate. Exponentially decaying average of the squared gradients. Generally, better than Adagrad.',
            adam: 'Adaptive leaning rate. Exponentially decaying first-order moment and second-order moment, and often used.',
            adamAlpha: 'Step size when update parameters.',
            adamBeta1: 'Rate with which first gradients decrease exponentially.',
            adamBeta2: 'Rate with which second gradients decrease exponentially.',
            learningRateMultiplier: 'Rate multiplied to learning rate',
            maxEpochs: 'Max epochs to iterate whole training dataset'
        },
        structureSearch: {
            structureSearchTitle: 'Structure Search. Once the seed network is given, it explores an optimizal network based on the seed.',
            iteratorLimit: 'Number of trials. Upper bound is X, the lower bound is Y.',
            method: 'Method to be used for Structure Search.',
            random: 'Random exploration.',
            networkFeatureGaussianProcess: 'It explores using Gaussian Process with the pre-defined features of a network structure.'
        }
    },
    result: {
        tradeOffGraph: 'Relationship among training errors, validation errors and multiplicative additions. Result of small validation error with small multiplicative additions is better.'
    },
    common: {
        backButton: 'Back',
        saveButton: 'Save',
        saveAsButton: 'Save as',
        undoButton: 'Undo',
        redoButton: 'Redo',
        cutButton: 'Cut',
        copyButton: 'Copy',
        pasteButton: 'Paste',
        runButton: 'Run training with the current configurations',
        profileButton: 'Measure the execution speed of the neural network under design in detail.',
        stopButton: 'Stop current training',
        evaluateButton: 'Run evaluation with a trained network.'
    }
}

export default tooltip_en